{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Environment import MazeEnvironment\n",
    "from Agent import RatAgent\n",
    "from maze_generator import maze_generator\n",
    "from ExperienceReplay import ExperienceReplay\n",
    "from policy_net import PolicyNet, Qloss\n",
    "import numpy as np\n",
    "import scipy.special as sp\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze = np.load('maze_generator/maze.npy')\n",
    "\n",
    "initial_position = [0,0]\n",
    "goal = [len(maze)-1, len(maze)-1]\n",
    "\n",
    "maze_env = MazeEnvironment(maze, initial_position, goal)\n",
    "maze_env.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_experiences_max = 10000\n",
    "n_experiences_min = 1000\n",
    "experiences = ExperienceReplay(capacity=n_experiences_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Rat Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat = RatAgent(environment=maze_env, memory_buffer=experiences, n_actions=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Policy net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNet(\n",
       "  (layerfc): Sequential(\n",
       "    (0): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layerfc2): Sequential(\n",
       "    (0): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer_no_activation): Linear(in_features=400, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net = PolicyNet(n_input_size=maze.size, n_actions=4, device=device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=5e-4)\n",
    "batch_size = 32\n",
    "gamma = 0.9\n",
    "\n",
    "policy_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20000\n",
    "cutoff = 2000\n",
    "\n",
    "#Prioritize exploration more in the beginning\n",
    "epsilon = np.exp(-np.arange(n_epochs)/(cutoff))\n",
    "epsilon[epsilon > epsilon[100*int(n_epochs/cutoff)]] = epsilon[100*int(n_epochs/cutoff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m batch \u001b[38;5;241m=\u001b[39m rat\u001b[38;5;241m.\u001b[39mexperiences\u001b[38;5;241m.\u001b[39mget_sample(batch_size\u001b[38;5;241m=\u001b[39mbatch_size, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     19\u001b[0m loss_t \u001b[38;5;241m=\u001b[39m Qloss(batch, policy_net \u001b[38;5;241m=\u001b[39m policy_net, gamma\u001b[38;5;241m=\u001b[39mgamma, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_t\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "win_count = 0\n",
    "lose_count = 0\n",
    "win_rate = 0\n",
    "for epoch in range(n_epochs):\n",
    "    loss = 0\n",
    "    counter = 0\n",
    "    current_epsilon = epsilon[epoch]\n",
    "    rat.isgameon = True\n",
    "    _ = rat.env.reset(current_epsilon)\n",
    "\n",
    "    while rat.isgameon:\n",
    "        rat.make_a_move(policy_net=policy_net, epsilon=current_epsilon, device=device)\n",
    "        counter += 1\n",
    "\n",
    "        if len(rat.experiences) < n_experiences_min:\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        batch = rat.experiences.get_sample(batch_size=batch_size, device=device)\n",
    "        loss_t = Qloss(batch, policy_net = policy_net, gamma=gamma, device=device)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        loss += loss_t.item()\n",
    "\n",
    "    if(rat.env.current_position == rat.env.goal_position).all():\n",
    "        result='won'\n",
    "        win_count += 1\n",
    "    else:\n",
    "        result='lost'\n",
    "        lose_count += 1\n",
    "    if (epoch+1)%1000 == 0:\n",
    "        rat.plot_policy_map(policy_net=policy_net, filename= \"policy_epoch\"+str(epoch+1)+\".jpg\", offset=[0.35,-0.3])\n",
    "    if win_count > 0 or lose_count > 0:\n",
    "        win_rate = win_count/(win_count+lose_count)\n",
    "    print(f\"Epoch: {epoch+1}/{n_epochs} | Loss = {loss:.4f} | Epsilon = {current_epsilon:.4f} |  Number of moves = {counter} | Win rate = {win_rate:.3f} | Wins = {win_count} | Loses = {lose_count}\")\n",
    "    clear_output(wait=True)\n",
    "    if epoch > 10000 and win_count > 10*lose_count:\n",
    "        if loss < 1e-3 and win_rate > 0.90:\n",
    "            torch.save(policy_net.state_dict(), \"best_policynet.torch\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"policynet.torch\")\n",
    "policy_net.eval()\n",
    "rat.isgameon = True\n",
    "rat.use_softmax = False\n",
    "_ = rat.env.reset(0)\n",
    "while rat.isgameon:\n",
    "    rat.make_a_move(policy_net, 0)\n",
    "    rat.env.draw()\n",
    "    clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
